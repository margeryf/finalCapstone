# -*- coding: utf-8 -*-
"""sentiment_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CVxtg-ZRqnFzEtEE12sFY9x02Y8Dv27W
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

path = "/content/drive/MyDrive/archive/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv"

amazon=pd.read_csv(path) #dataset is now stored in a pandas dataframe
amazon.head(5) #check if it is working



import numpy as np
import spacy

from textblob import TextBlob  #additional ext. of spacy - for sentiment analysis - spacy textblob
from wordcloud import WordCloud #terminal  py -m pip install wordcloud #dont have to use -  it is like a visualization - i can see which
import matplotlib.pyplot as plt #need to install!!  #terminal  py -m pip install matplotlib #optional

#Cleaning the Data: for sentiment analysis i need : reviews.text + reviews.title + reviews.username

cleaned = amazon[["reviews.text","reviews.title","reviews.username"]]
cleaned

#check if there are any no-values
cleaned.isnull().sum()

cleaned.dropna(inplace=True, axis=0) #drop the values

cleaned.isnull().sum() #double checking there aren't nul values

#Preprocess Data

text = cleaned["reviews.text"]
text

import spacy.cli
spacy.cli.download("en_core_web_md")
nlp = spacy.load("en_core_web_md")

#Preprocessing

def preprocess(text): # Remove stopwords and punctuation

    doc = nlp(text.lower().strip()) #removing and cleaning the text
    processed = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct] # remove stopwords +  use lower so it is consistent

    return " ".join(processed) #joining the new words

#getting the text (the text now it is an array) - i need to use applied method

cleaned['processed.text'] = cleaned['reviews.text'].apply(preprocess) #this is gonna preprocessed everything

cleaned.head() #now i can see processed.text

#Data has been preprocessed, next step is sentiment analysis

from collections import defaultdict

positive_words= defaultdict(int)
negative_words= defaultdict(int)

#Define function for sentiment analysis

for sentence in cleaned['processed.text']:

  doc = nlp(sentence)
  tokens = [token.lemma_.lower().strip() for token in doc if not token.is_stop and token.is_alpha]

  for token in tokens:

    blob = TextBlob(str(token)) #using TextBlob / polarity = float that lies between [-1,1], -1 indicates negative sentiment and +1 indicates positive sentiments
    polarity = blob.sentiment.polarity

    if polarity > 0:
      positive_words[token.lower()] += 1
    elif polarity < 0:
        negative_words[token.lower()] += 1

#Test the sentiment Analysis - using WordCloud

pos_wordcloud = WordCloud(width=600, height=400, background_color='red').generate_from_frequencies(positive_words)
neg_wordcloud = WordCloud(width=600, height=400, background_color='red').generate_from_frequencies(negative_words)

fig, ax = plt.subplots(1, 2, figsize=(10, 5))

ax[0].imshow(pos_wordcloud, interpolation = "bilinear")
ax[0].set_title("Positive Words")
ax[0].axis("off")

ax[1].imshow(neg_wordcloud, interpolation = "bilinear")
ax[1].set_title("Negative Words")
ax[1].axis("off")

plt.show()